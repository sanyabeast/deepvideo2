# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ðŸŽ² SAMPLE CONFIGURATION FILE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# This is a sample configuration file for DeepVideo2.
# Copy this file and modify it for your own projects.
# 
# NOTE: The filename (without extension) will be used as the project name.
# For example, if you name this file "motivation.yaml", the project name will be "motivation".

# LLM Configuration
llm:
  default_model: "meta-llama-3.1-8b-instruct"  # Your local LLM model name
  seed: 0  # Will be randomized at runtime if not specified

# Voice Generation Configuration
voice:
  zonos_tts_server: "http://localhost:5001/generate"  # URL to your TTS server
  voice_samples:
    - "C:\\path\\to\\voice\\sample1.mp3"  # Path to voice sample files
    - "C:\\path\\to\\voice\\sample2.mp3"
  speech_rate: "15"  # Speech rate for TTS (higher = faster)
  normalization:
    target_db: -20.0  # Target dB level for audio normalization
    enabled: true     # Whether to automatically normalize generated voice lines
  silence_trimming:
    enabled: true     # Whether to trim silence from the end of voice lines
    max_silence_sec: 1.0  # Maximum silence to keep at the end in seconds
    threshold_db: -50  # Threshold in dB below which audio is considered silence

# Video Generation Configuration
video:
  imagemagick_binary: "C:\\path\\to\\ImageMagick\\magick.exe"  # Path to ImageMagick binary
  background_music_volume: 0.2  # Volume multiplier for background music (0.0 to 1.0)
  voice_narration_volume: 1.0   # Volume multiplier for voice narration (0.0 to 1.0)
  use_consistent_font: true     # Use the same font for all slides in a scenario
  intro_delay: 1.0              # Delay in seconds before the first slide appears
  outro_delay: 2.0              # Delay in seconds after the last slide before the video ends
  emoji_enabled: true           # Whether to render emojis in videos (true = enabled, false = disabled)
  emoji_font: "lib/noto_sans_emoji.ttf"  # Path to emoji font file (relative to project root)
  emoji_scale: 1.5              # Scale factor for emoji size (1.0 = normal, 1.5 = 50% larger)
  emoji_rotation:
    enabled: true               # Whether to randomly rotate emojis
    min_angle: -30              # Minimum rotation angle in degrees
    max_angle: 30               # Maximum rotation angle in degrees

# Image Generation Configuration
images:
  comfy_server_address: "127.0.0.1:8188"  # URL to the ComfyUI server
  steps: 12  # Number of steps for image generation
  default_negative_prompt: "text, watermark, signature, blurry, distorted, low resolution, poorly drawn, bad anatomy, deformed, disfigured, out of frame, cropped"  # Default negative prompt
  generation_timeout: 60  # Maximum wait time in seconds for image generation
  # The workflow is a JSON representation of the ComfyUI workflow
  # This is a basic example using Flux model, but you can export your own workflow from ComfyUI
  # Special placeholders:
  # {PROMPT} - Will be replaced with the slide's background_image_description
  # {NEGATIVE_PROMPT} - Will be replaced with the default_negative_prompt
  # {SEED} - Will be replaced with a random seed
  # {STEPS} - Will be replaced with the steps value
  workflow: |
    {
      "5": {
        "inputs": {
          "width": 1024,
          "height": 1024,
          "batch_size": 1
        },
        "class_type": "EmptyLatentImage",
        "_meta": {
          "title": "Empty Latent Image"
        }
      },
      "6": {
        "inputs": {
          "text": "{PROMPT}",
          "clip": [
            "11",
            0
          ]
        },
        "class_type": "CLIPTextEncode",
        "_meta": {
          "title": "CLIP Text Encode (Prompt)"
        }
      },
      "8": {
        "inputs": {
          "samples": [
            "13",
            0
          ],
          "vae": [
            "10",
            0
          ]
        },
        "class_type": "VAEDecode",
        "_meta": {
          "title": "VAE Decode"
        }
      },
      "9": {
        "inputs": {
          "filename_prefix": "flux/image",
          "images": [
            "8",
            0
          ]
        },
        "class_type": "SaveImage",
        "_meta": {
          "title": "Save Image"
        }
      },
      "10": {
        "inputs": {
          "vae_name": "ae.safetensors"
        },
        "class_type": "VAELoader",
        "_meta": {
          "title": "Load VAE"
        }
      },
      "11": {
        "inputs": {
          "clip_name1": "t5xxl-fp8-e4m3fn.safetensors",
          "clip_name2": "clip-l.safetensors",
          "type": "flux",
          "device": "default"
        },
        "class_type": "DualCLIPLoader",
        "_meta": {
          "title": "DualCLIPLoader"
        }
      },
      "12": {
        "inputs": {
          "unet_name": "flux.1s-fp8.safetensors",
          "weight_dtype": "default"
        },
        "class_type": "UNETLoader",
        "_meta": {
          "title": "Load Diffusion Model"
        }
      },
      "13": {
        "inputs": {
          "noise": [
            "25",
            0
          ],
          "guider": [
            "22",
            0
          ],
          "sampler": [
            "16",
            0
          ],
          "sigmas": [
            "17",
            0
          ],
          "latent_image": [
            "5",
            0
          ]
        },
        "class_type": "SamplerCustomAdvanced",
        "_meta": {
          "title": "SamplerCustomAdvanced"
        }
      },
      "16": {
        "inputs": {
          "sampler_name": "euler"
        },
        "class_type": "KSamplerSelect",
        "_meta": {
          "title": "KSamplerSelect"
        }
      },
      "17": {
        "inputs": {
          "scheduler": "simple",
          "steps": {STEPS},
          "denoise": 1,
          "model": [
            "12",
            0
          ]
        },
        "class_type": "BasicScheduler",
        "_meta": {
          "title": "BasicScheduler"
        }
      },
      "22": {
        "inputs": {
          "model": [
            "12",
            0
          ],
          "conditioning": [
            "6",
            0
          ]
        },
        "class_type": "BasicGuider",
        "_meta": {
          "title": "BasicGuider"
        }
      },
      "25": {
        "inputs": {
          "noise_seed": {SEED}
        },
        "class_type": "RandomNoise",
        "_meta": {
          "title": "RandomNoise"
        }
      }
    }

# Media Options
media_options:
  music_files_count: 16  # Number of music files to show in scenario generation
  video_files_count: 16  # Number of video files to show in scenario generation

# Emotions List
emotions:
  - "happiness"
  - "sadness"
  - "disgust"
  - "fear"
  - "surprise"
  - "anger"
  - "neutral"

# Prompt Configuration
prompts:
  # Topics generation prompt
  topics:
    role: "You are a creative and insightful motivational content strategist for short-form video platforms (TikTok, Reels, YouTube Shorts)."
    instruction: "Generate a list of 5 **fresh and unconventional motivational video topics**. Avoid clichÃ©, overused ideas like \"overcoming fear\" or \"imposter syndrome\". Be unique, thought-provoking, and modern."
    requirements:
      - "Be specific and catchy"
      - "Appeal to younger audiences (Gen Z, Millennials)"
      - "Spark curiosity or emotion"
      - "Be suitable for short, powerful videos"
      - "Use only plain ENGLISH words and phrases."
      - "Do NOT include emojis, icons, or non-standard symbols."
      - "Do NOT use non-English languages or phrases."
  
  # Scenario generation prompt
  scenario:
    role: "You are a short-form video copywriter crafting motivational videos for TikTok, Reels, and YouTube Shorts."
    instruction: "Create a **micro-story split into short slides** (like Instagram story frames) for the topic: \"{topic}\""
    constraints:
      - "Total video duration: **8-16 seconds max**"
      - "Each slide: **very short sentence or phrase**, 3â€“8 words"
      - "Each slide duration: **1 to 4 seconds** (max)"
      - "Max 8 slides"
    style:
      - "Ultra-punchy and emotionally charged"
      - "Hooks the viewer in slide 1"
      - "Builds emotional or motivational momentum"
      - "Uses rhythm, repetition, shock, or questions"
      - "Ends strong with a bold punchline or insight"
      - "Use everyday, casual Gen Z language"
      - "Use only plain ENGLISH for all text."
      - "Do NOT include emojis, emoticons, icons, or non-standard symbols."
      - "Do NOT include non-English words, slang from other languages, or cultural references that may require translation."

# Directory Configuration
directories:
  # Input directories
  videos_dir: "lib/videos"
  music_dir: "lib/music"
  fonts_dir: "lib/fonts"
  emoji_fonts_dir: "lib/fonts_emoji"
  
  # Output directories (will be created under output/{project_name}/)
  scenarios: "scenarios"
  voice_lines: "voice_lines"
  output_videos: "videos"
  images: "images"
